---
title: "DS 320: Predicting Hockey Contracts"
author: 
  - "Brian Ellis"
  - "Luke Kerwin"
  - "Eric Wu"
  - "Griffin Jordan"
format: pdf
editor: visual
date: "December 5, 2023" # This will automatically insert the current date
---

\newpage

# Abstract

This report encapsulates our approach to predicting National Hockey League (NHL) player contract values by utilizing a sophisticated data integration and machine learning pipeline. Our study primarily focuses on integrating and analyzing player performance data and historical contract information, spanning from 2009 to 2022. The data is meticulously sourced from [CapFriendly](https://www.capfriendly.com/) and [Hockey Reference](https://www.hockey-reference.com/) through Python web scraping techniques.

A key aspect of our methodology involves rigorous data cleaning and pre-processing, where we exclude Entry-Level contracts (ELCs), Restricted Free Agent contracts (RFAs), and Contract Extensions to refine the dataset for more accurate model training. These three types of contracts are restricted in ways that would skew our modeling. This cleaned data is then stored in a locally hosted [MySQL](https://www.mysql.com/) database, with a [Flask](https://flask.palletsprojects.com/en/3.0.x/) and SQLAlchemy-built API facilitating remote data access, demonstrating the potential for online deployment.

The cornerstone of our project is the development of a predictive model, derived from the merged dataset of contracts and performance statistics. This model, encapsulated as a Python class, is integrated into our API, enabling real-time predictions as player statistics update.

Further, we have designed an interactive dashboard using [Tableau](https://www.tableau.com/), connected directly to our MySQL database, to visually represent our findings and predictions. While currently limited to manual updates due to resource constraints, the dashboard is designed for potential future automation and easy management, should the [Tableau Server](https://www.tableau.com/products/server) be utilized.

Last but not least, we decided to try replicating the results using Snowflake API and demonstrate cloud-based deployment as our new approach. As a result of the approach, we successfully integrated the player performance data and historical contract information using the Snowflake API, created an interactive dashboard that connects directly to the Snowflake warehouse, and implemented the machine learning models using Snowpark packages and sessions.

This report not only presents a viable model for predicting NHL player contracts but also exemplifies the effective use of data integration, machine learning, and visualization techniques in sports analytics. The implications of this study extend beyond contract predictions, offering insights into data-driven decision-making in professional sports.

\newpage

# Table of Contents

## [1. Introduction](#introduction-hl)

## [2. Methodology](#methodology-hl)

## [3. Conclusion](#conclusion-hl)

## [4. Contribution](#contribution-hl)

## [5. References & Appendices](#references-hl)

\newpage

# Introduction {#introduction-hl}

In an era where data reigns supreme, sports analytics has emerged as a cornerstone of strategic decision-making in professional sports. The ability to parse through vast amounts of data and extract meaningful insights is invaluable, particularly in the high-stakes world of professional hockey. This paper delves into the realm of data integration within sports analytics, with a focus on the National Hockey League (NHL).

Data integration, the process of combining data from different sources into a unified view, plays a pivotal role in modern sports analytics. It enables a comprehensive analysis of player performance, a crucial factor in determining contract values in the NHL. However, predicting player contracts is a complex task, riddled with challenges due to the dynamic and multifaceted nature of player performance metrics and contract negotiations.

Our project confronts this challenge head-on. We aim to predict NHL player contracts by leveraging a decade's worth of player performance data (2009-2022) and historical contract information, sourced from CapFriendly and Hockey Reference By integrating these diverse datasets, we seek to unveil patterns and correlations that influence contract values.

The objectives of this study are twofold: firstly, to develop a robust data integration pipeline that can efficiently process and merge diverse data types; and secondly, to create a predictive model that can accurately forecast NHL contract values based on player performance data.

The significance of this study extends beyond the immediate realm of contract prediction. It serves as a case study in the effective application of data integration techniques in sports analytics, potentially paving the way for similar analyses in other sports disciplines.

This paper is structured as follows: we begin with a comprehensive methodology section outlining our data collection, cleaning, and integration processes. This is followed by a detailed account of our model development and implementation strategies. We then present our results and discuss their implications, before concluding with key takeaways and potential avenues for future research.

\newpage

# Methodology {#methodology-hl}

## 1. Data Collection

### Sources

-   **CapFriendly**
    -   Used to gather all historic NHL contract signings from 2012-2022.
    -   Used [BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/) to scrape each table month by month due to the website's web-scraping prevention tools.
    -   Specific URL [here](https://www.capfriendly.com/signings)
-   **Hockey Reference**
    -   2009-2022 due to needing n-3 previous seasons of statistics to predict year n
    -   Much easier to scrape than CapFriendly due to the tables being plain HTML tables.
    -   Benefited from using [Pandas](https://pandas.pydata.org/)' `read_html()` function.
    -   Specific URL [here](https://www.hockey-reference.com/leagues/NHL_2022_skaters.html)

### Web scraping Techniques

During the process of collecting data some insights we gained were profound in how we set up and deployed our data pipeline. For example, normally you can retrieve data by changing the URL parameters. In this case, the web URL was almost a dummy URL. We later found an Ajax request call that had similar parameters in it. The next challenge was adapting to the pagination. If we ever called a page that didn't exist, the code would instantly break and we would lose our data. To combat this, we were able to use caching and try-except statements to safeguard our data collection.

With the Hockey Reference data source since the statistics were not paginated, we didn't have to iterate from month to month. We could get a whole year's data in one request call. All we had to do was change the year in the URL; https://www.hockey-reference.com/leagues/NHL_2022_skaters.html. One unique thing about this dataset was that the HTML column headers were built into the actual dataset. We were able to handle this in the Data Cleaning and Preprocessing stage.

Python code used to web scrape can be found [here](https://github.com/lukekerwin/DS-320_Project/blob/main/Luke/data.py).

\newpage

## 2. Data Cleaning and Preprocessing

### Contract Data

![Contract Raw Data](/Users/lukekerwin/Library/CloudStorage/OneDrive-ThePennsylvaniaStateUniversity/School/DS%20320/DS-320_Project/Report%20Images/ContractRawData.png)

Above is what the contract data looked like as soon as we scraped it from CapFriendly. Some key features that we need to clean and preprocess are:

1.  Handling `PLAYER.1` column
2.  Standardizing the `DATE` to a more accessible data point
3.  Filter `TYPE` to `Stnd (UFA)` to get the true contracts
4.  Replacing `EXTENSION` values with binary (0,1)
5.  Standardizing `VALUE` and `CAP HIT` to integers from strings
6.  Adding an `ID` column for easy database storage

After implementing these changes our final contract dataset looks like this:

![Contract Cleaned Data](/Users/lukekerwin/Library/CloudStorage/OneDrive-ThePennsylvaniaStateUniversity/School/DS%20320/DS-320_Project/Report%20Images/ContractCleanedData.png)

\newpage

### Statistics Data

![Statistics Raw Data](/Users/lukekerwin/Library/CloudStorage/OneDrive-ThePennsylvaniaStateUniversity/School/DS%20320/DS-320_Project/Report%20Images/StatisticsRawData.png)

As you can see from above, there are some key changes we will need to make to make our data friendly not only for machine learning but also for storing in our SQL database. Here is what we did:

1.  Remove `Rk` as it has no value to us
2.  Use `TOT` (Team) rows for players who played on multiple teams in one season (example: Craig Adams above)
3.  Fix duplicate column names such as `EV`, `PP`, `SH`
4.  Fix column names with % in them as they are not MySQL-compliant names
5.  Add an `ID` column
6.  Remove goalie statistics

![Statistics Cleaned Data](/Users/lukekerwin/Library/CloudStorage/OneDrive-ThePennsylvaniaStateUniversity/School/DS%20320/DS-320_Project/Report%20Images/StatisticsCleanedData.png)

Python code used to clean and preprocess the data can be found [here](https://github.com/lukekerwin/DS-320_Project/blob/main/Luke/data.py).

After formatting and filtering our data, we have `1006` contract observations and `9186` statistical observations to support.

Now we need to store the data.

\newpage

## 3. Database Setup and Management

Due to familiarity and accessibility of documentation, we chose to go with a MySQL database. It's a very well-known DB and has a ton of support for some of the features we wanted to add later on.

### Setup

Setup was fairly easy thanks to the Python packages; Flask and sqlalchemy. These packages allowed us to essentially create the DB base from our panda's data frames. Later we were able to go in and customize the relationships such as primary keys and foreign keys. Here are the two table schemas:

\begin{figure}[h]
  \centering
  \begin{minipage}{0.45\textwidth}
    \centering
    \includegraphics[width=\linewidth]{/Users/lukekerwin/Library/CloudStorage/OneDrive-ThePennsylvaniaStateUniversity/School/DS 320/DS-320_Project/Report Images/ContractsSchema.png}
    \caption{Contract Schema}
    \label{fig:image1}
  \end{minipage}\hfill
  \begin{minipage}{0.45\textwidth}
    \centering
    \includegraphics[width=\linewidth]{/Users/lukekerwin/Library/CloudStorage/OneDrive-ThePennsylvaniaStateUniversity/School/DS 320/DS-320_Project/Report Images/StatisticsSchema.png}
    \caption{Statistics Schema}
    \label{fig:image2}
  \end{minipage}
\end{figure}

\newpage

### Hosting Locally

Something cool, that we didn't discover until working on this project is that MySQL integrates well with Mac OSX which happened to be the operating system we hosted it on. Here you can manage the local instance with ease.

![Mac OSX MySQLConfig](/Users/lukekerwin/Library/CloudStorage/OneDrive-ThePennsylvaniaStateUniversity/School/DS%20320/DS-320_Project/Report%20Images/MySQLConfig.png)

If this project ever needed to be hosted online, MySQL allows you to export the DB config which allows for an easy and seamless migration from local to online.

## 4. Data Integration

### Merging the Data

Due to the time restraint we set on the statistics data to predict the contract value, the merging of the two datasets isn't as easy as originally thought. For example. Say a player has played 10 seasons from 2005-2015. If we want to predict his 2012 contract, we need to filter the data down to the 2009, 2010, and 2011 seasons. It might not seem like a big issue, but we need to do that step for each player we are predicting as well.

To overcome this challenge we utilize the Pandas package in Python to do a selective group-by so that we are only using the data from our restricted timeline and not the entire data set. We also want to standardize the `POS` column into positional groups. This will aid the predictive model in being able to separate centers, wingers, and defensemen from each other. We achieve this by using a simple if-then function in Python.

The code used to merge the data can be found [here](https://github.com/lukekerwin/DS-320_Project/blob/main/Luke/ml.py)

### Handling Heterogeneous Data

At first, we were worried that we were going to run into issues with heterogenous data, but as we worked through cleaning and merging the data, we ended up with homogenous data. That being said, in the future, there may come a time when two people have the same name or they play the same position in the same year. A new way to handle the heterogeneous data will need to be implemented. We would recommend either manually assigning a unique identifier for each player or getting access to data that has personal information that could be used to distinguish one from another.

After completing all of the steps necessary to merge the `contract` and `statistics` tables we are left with a dataset that looks like this:

![Merged Dataset](/Users/lukekerwin/Library/CloudStorage/OneDrive-ThePennsylvaniaStateUniversity/School/DS%20320/DS-320_Project/Report%20Images/MergedDataset.png)

If you look closely you can also see that we decided to standardize all of the statistics columns to a per-game basis so that players who played more games wouldn't be seen as more valuable than those who played less.

\newpage

## 5. ML Model Development

After extensive model testing, we concluded that a Voting Regressor is the best-suited model for predicting NHL contracts. The model performed the best in both of our testing metrics; r-squared and RMSE. Despite the Support Vector Machine performing better in both r-squared and RMSE, the Voting Regressor is more robust due to the aggregation of multiple models. After feature selection and adjusting hyper-parameters, we were able to get the model to predict contracts within roughly \$1,000,000. This produced an r-squared value of `0.726` and the exact RMSE is `1,131,716`. Here are the results of all of our tests:

![Modeling Results](/Users/lukekerwin/Library/CloudStorage/OneDrive-ThePennsylvaniaStateUniversity/School/DS%20320/DS-320_Project/Report%20Images/MLresults.png)

### Integration

After we selected our model, we were able to integrate the model directly into our API through an endpoint/route. The user can select which season they want to see predictions for by using `?season=` in the URL of the request. The API then returns real-time predictions using the pre-trained model, in seconds. This would come in handy if you needed to access the predictions from other external sources than just a dashboard.

\newpage

## 6. API Integration

### Why?

At first, we did not have any plans for an API to be built into our pipeline. However, we kept running into an issue as we were working together as a team; getting the most recent data from group member to group member. Whenever someone would make a change to the data cleaning and pre-processing, we needed to move that data to everyone else who was working on other sections that used the same data.

At first, we would download the data to CSVs and send them via email. At this moment we realized we needed a much better way to solve our problem. After some research, we concluded that an API would be best, as it could connect directly to our main data source (MySQL DB) and we would not have to worry about conflicts due to changing CSVs.

The original intent was to just have two routes/endpoints; one for the contract data and one for the statistics data. However, after reading the Flask documentation, we discovered there was so much more that we could do. Not only could we send and receive data, but we could also connect our machine-learning algorithm to an endpoint to allow for quick real-time predictions. While it may be overkill for the scale of this project, it certainly is an advancement in data integration that would apply in the real world.

Due to hosting expenses, we aren't able to host the API publicly, but [here](https://youtu.be/sLFL2RYb8PM) is a video that quickly demonstrates its capabilities.

\newpage

## 7. Visualization and Dashboard

### Tool Choice

There were a lot of options to pick from when selecting a visualization tool. However, we ended up going with Tableau as not only was it familiar to our group, but also had the best configuration for integrating without data. One of the key features was that it allowed us to connect our MySQL database as a direct data source which allowed for easy updating of data. Tableau also offers free hosting of dashboards on its Tableau Public service. These two key factors made it an easy choice over other options such as; matplotlib, ggplot, and others.

### Dashboard Design

![Dashboard](/Users/lukekerwin/Library/CloudStorage/OneDrive-ThePennsylvaniaStateUniversity/School/DS%20320/DS-320_Project/Report%20Images/Dashboard.png)

Creating the dashboard was a unique process. We wanted to capture the insights gained from not only our machine learning algorithm but also the effectiveness of our pipeline. The left half of the dashboard has each prediction with features that give insight into how accurate our prediction was. The top right plot displays each prediction but allows for direct comparison with other similar predictions. Finally, in the bottom right, we have the predictions broken down by team. This allows for quick analysis of what teams might have had the best contract signings versus the worst. The dashboard is also interactive. The user can select which season's data they would like to view as well as the age of the players.

\newpage

### Database Connection

Arguably the most valuable feature that Tableau has to offer is the ability to connect directly to our MySQL database. All we had to do was download a connector plugin and log in to the local MySQL server.

![MySQL / Tableau Integration](/Users/lukekerwin/Library/CloudStorage/OneDrive-ThePennsylvaniaStateUniversity/School/DS%20320/DS-320_Project/Report%20Images/TableauData.png)

As you can see above, the MySQL database seamlessly integrates into Tableau and allows for a ton of customization. One of the features we didn't need to implement but was available was the Custom SQL. This allows for the creation of new tables using SQL. Think of it almost as a view. You can also draw relationships between the tables inside Tableau to allow for easy fluidity in the data.

The interactive dashboard is hosted [here](https://public.tableau.com/app/profile/luke.kerwin/viz/DS320/Dashboard1) thanks to the free hosting via Tableau Public.

\newpage

## 8. Extra: Cloud-based Data Warehousing Exploration

### Integration in Snowflake

Integration in Snowflake is very straightforward as it's also based on SQL queries. The level of difficulty of integration is simpler than what we used in the Jupyter Notebook given that Snowflake allows importing CSV files so we don't have to create the schema ourselves. In addition, it's very easy to create new tables in Snowflake as there's the create table keyword can be directly used on top of a SQL query.

### Data Storage in Snowflake

The database schema is stored in a Snowflake virtual warehouse that ultimately supports the execution of SQL queries and allows external notebook files to access the schema in Snowflake sessions using Snowpark packages. Aside from the local notebook file, there are also collaborative companies like Hex that provides online notebook and can directly access the schema by establishing a connection.

### Snowflake Dashboard

Snowflake provides the user easy access to create a simple dashboard using SQL queries or Python queries. The dashboard that we created for our data supports an analysis of the relationship between player position and a variety of other variables presented in different data visualizations.

![Snowflake Dashboard](/Users/lukekerwin/Library/CloudStorage/OneDrive-ThePennsylvaniaStateUniversity/School/DS 320/DS-320_Project/Report Images/sdashboard.png)

### Machine Learning in Snowflake

Adding machine learning to our Snowflake enviornment was actually rather easy. Snowflake has "worksheets" which are really just notebooks. Here we were able to use the exact same code from our previous ML model and run it on the data in our Snowflake DB. Heres a view of what the worksheets look like:

### Disadvantage of Snowflake

\begin{figure}[h]
  \centering
  \includegraphics[width=4.5in]{/Users/lukekerwin/Library/CloudStorage/OneDrive-ThePennsylvaniaStateUniversity/School/DS 320/DS-320_Project/Report Images/worksheet.png}
  \caption{Snowflake Worksheet}
  \label{fig:image2}
\end{figure}

The biggest disadvantage of using Snowflake is that it can be costly, the cost of storing our data is about \$3.00 per day. Other than the cost, the only disadvantage is the low customization capability in Snowflake API. However, this could be due to our limited experience with Snowflake.

\begin{figure}[h]
  \centering
  \includegraphics[width=4.5in]{/Users/lukekerwin/Library/CloudStorage/OneDrive-ThePennsylvaniaStateUniversity/School/DS 320/DS-320_Project/Report Images/Usage.png}
  \caption{Snowflake Usage}
  \label{fig:image2}
\end{figure}

\newpage

# Conclusion {#conclusion-hl}

### What We Learned

Over the course of the semester, our project plan remained consistent, but our methods and approach fluctuated quite a bit. At one point we realized we didnt have the right data needed to predict the contract values accurately. Once we switched to the right data source, we had to deal with web-scraping preventives that were used by the website. After some research we were able to get the data in a safe and efficient way. We also ran into a lot of issues with integrating Snowflake. There wasn't a ton of documentation on the issues that were coming up, so it took longer than expected to overcome. Overall, we learned a lot of new techniques to integrate not only different kinds of data sources, but also different tools that improve the overall experience of using our product.

### Next Steps

If we were to continue this project further, the next steps we would take are hosting the API and database publicly, adding in more player information data to help with predictions, and finally adding contextual data like market trends and team spending habits. Hosting the API and database would be pretty simple, Amazon Web Services offers some pretty solid options at entry level prices. As for player information data, having a history of player injuries would be really interesting to add. Unfortunately in our own research, we were unable to find a reliable source. Lastly, market data is also hard to find as the NHL is not as well covered as other leagues like the NFL and NBA.

# Contribution {#contribution-hl}

| Student | Contributions |
|---|---|
| Luke Kerwin | Data Collection and Storage |
| Brian Ellis | Data Cleaning and Machine Learning |
| Eric Wu | Snowflake Integration and Report Writing |
| Griffin Jordan | Data Cleaning and Report Writing |

\newpage

# References & Appendices {#references-hl}

[Github Repo](https://github.com/lukekerwin/DS-320_Project)

- All code is in the "Code" folder broken down by file

**Data Sources**

1. [CapFriendly](https://www.capfriendly.com/)

2. [Hockey Reference](https://www.hockey-reference.com/)
