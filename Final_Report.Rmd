---
title: "Predicting Hockey Contracts"
author: "Eric Wu, Luke Kerwin, Brian Ellis, Griffin Jordan"
date: "`r Sys.Date()`"
header-includes:
    - \usepackage{setspace}\doublespacing
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.pos = "H")
```

# Introduction
|       In the dynamic and competitive world of professional sports, where every move can make or break a team's fortunes, the business of player contracts stands as a pivotal aspect that demands precision and foresight. Our group took on the challenge of developing a machine-learning model capable of predicting the financial value of a one-year hockey player contract.
|       The professional sports industry, particularly the National Hockey League (NHL), has witnessed an unprecedented evolution in recent years. As teams increasingly rely on data-driven decision-making processes, the need for predictive models to estimate player contract values has become more apparent. The ability to anticipate the financial implications of signing or retaining a player not only facilitates effective roster management but also plays a crucial role in maintaining the financial health and sustainability of a team.
|       With the use of a multitude of player performance metrics and historical data, the goal was to create a model that could predict the size of a player's contract for a desired year. This report delves into the methodologies employed in curating and preparing the data, selecting the appropriate features, and fine-tuning the machine learning model. We navigated through a few challenges, from data availability and quality to data integration.
# Datasets
|       To create a model that could accomplish this task, the first step is to retrieve all the data that impacts a player's future contract. This includes all types of player informational data such as age, height, and weight. We would also need a player's historical data which is comprised of all the in-game performance statistics throughout their entire career. The first dataset we used was from a website named CapFriendly. CapFriendly is a website that provides information and tools related to the salary cap in professional hockey leagues, particularly the National Hockey League (NHL). It offers users the ability to explore and analyze team rosters, player contracts, salary cap details, and other financial aspects of hockey teams. The site is a valuable resource for fans, analysts, and anyone interested in understanding the financial aspects of NHL teams and player contracts.

![CapFriendly Dataset Sample](/Users/wue77/Documents/GitHub/DS-320_Project/Image/one.png)

|       The dataset used for this project is a representation of hockey player contracts, providing information such as player names, ages, positions, teams, contract dates, contract types, contract structures (e.g., Entry-Level Contract or Standard), contract lengths, total contract values, and annual salary cap hits. Each row corresponds to a different player's contract, with details about their contractual terms and financial aspects. For example, it includes information on Entry-Level Contracts (ELC), Standard (UFA) contracts, two-way contracts, and the financial figures associated with each player's deal.
|       For the player statistical information, we acquired data from a website called Hockey Reference. Hockey Reference serves as a statistical and historical resource for ice hockey. It provides a wide range of information on players, teams, and leagues, particularly focused on the NHL. On this site, users can find detailed statistics, player profiles, team records, and other data related to the sport.

![Hockey Reference Data set Sample](/Users/wue77/Documents/GitHub/DS-320_Project/Image/two.jpg)

|       This dataset summarizes the performance of individual hockey players in different seasons from 2009 to 2023. It includes various statistics such as player details (name, age, position), team information, game-related data (games played, goals, assists, points), and additional performance metrics like plus-minus, shooting percentage, time on ice, blocks, hits, and faceoff statistics, etc. Each row corresponds to a specific player's performance in a particular season.  
# Data Analysis / Method
|       For our data analysis section, we decided to implement two different approaches for safety purposes.
## Regular Approach
|       In the regular procedure, we integrated the source data using programming languages like Python and R and cleared the joint data based on our needs. Afterward, we implemented a MySQL database and designed a local API to easily access the database in the local environment. The API can be accessed globally if the API and the database get hosted. 

![Creating a contract table using Python](/Users/wue77/Documents/GitHub/DS-320_Project/Image/three.jpg)

|       Above is a sample code that we used to generate the contract table in MySQL using SQLAlchemy, and we did the same thing with the player statistics data. Next, we ran a series of machine-learning models in a local Jupyter notebook to predict the contract length and player salary. The machine learning models included are linear regression, random forest, support vector machine (SVM), gradient boosting, and ensemble method using a voting regressor. Among the various models, SVM performs best with a mean square error of around a million, which means we were able to predict the salary in the million range. Lastly, we created a dashboard using the model and tableau. The dashboard allows the user to examine the prediction and the actual contract value. 

![Tableau dashboards](/Users/wue77/Documents/GitHub/DS-320_Project/Image/four.jpg)

## Snowflake
|       Besides the regular procedure, we also decided to try a cloud-based storage approach by repeating the procedures in Snowflake. This time, we integrated the source by performing SQL queries in the Snowflake environment and created a dashboard based on the joint table.

![Tableau dashboards](/Users/wue77/Documents/GitHub/DS-320_Project/Image/five.jpg)

|       The dashboard examines some relationships between the position variable and other important variables. In addition to the dashboard, we also created a Jupyter Notebook that loads data from the Snowflake warehouse and performs the machine methods on the same data. 


# Conclusion

# Contribution

Eric Wu – Report writing, integration of the data using Snowflake and R, created Snowflake dashboard and data visualization.  
Luke Kerwin – Scrape web data, build SQL database and API, and implement the model in Snowflake environments.   
Brian Ellis – Integration of the data using Python, implementation of various machine learning methods, and created data visualizations on the result.   
Griffin Jordan – Report writing  

# Appendix

GitHub Repository - https://github.com/lukekerwin/DS-320_Project
























