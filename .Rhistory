center = TRUE,
scale = TRUE)
View(scaled_data)
# Doing a 8/2 training, testing splits
set.seed(123)
train_ind <- sample(1:nrow(scaled_data), floor(0.8*nrow(scaled_data)))
set.seed(NULL)
train <- scaled_data[train_ind, ]
test <- scaled_data[-train_ind, ]
View(test)
rf <- randomForest(charges ~ ., data = train, ntree = 500, mtry = 3)
# Create a vector of prediction probability
pred_rf <- predict(rf, newdata = test)
# Calculate RSS
RSS_rf <- exp(sum(test$charges - pred_rf)^2)
knitr::opts_chunk$set(echo = TRUE)
# Read the data
sleep <- read.csv("sleep.csv")
# Convert blood pressure to two numerical scales
sleep <- sleep %>%
separate(Blood.Pressure, c("place1", "place2")) %>% # Separate blood pressure into systolic and diastolic measures
mutate(
Systolic = as.integer(place1),
Diastolic = as.integer(place2)
) %>%
select(-place1, -place2)
knitr::opts_chunk$set(echo = TRUE)
View(sleep)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
# Read the data
sleep <- read.csv("sleep.csv")
# Convert blood pressure to two numerical scales
sleep <- sleep %>%
separate(Blood.Pressure, c("place1", "place2")) %>% # Separate blood pressure into systolic and diastolic measures
mutate(
Systolic = as.integer(place1),
Diastolic = as.integer(place2)
) %>%
select(-place1, -place2)
sleep %>%
filter(Age == 27)
sleep %>%
filter(Age == 28)
sleep %>%
filter(Age == 29)
sleep %>%
filter(Age == 30)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(glmnet)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(glmnet)
#Read in data
Credit <- read.csv("Credit.csv")
model1a <- lm(Balance ~ Income + Limit + Rating + Cards + Age, data = Credit)
summary(model1a)
coef(model1a)
#Convert Income to dollars
Credit <-
Credit %>%
mutate(Income_dollars = Income * 1000)
model1b <- lm(Balance ~ Income_dollars + Limit + Rating + Cards + Age, data = Credit)
summary(model1b)
coef(model1b)
coef(model1a)
coef(model1b)
Credit <- read.csv("Credit.csv")
#Put data frame in form needed for glmnet
Xmat <- model.matrix(Balance ~ . , data=Credit)[ ,-1]
y <- Credit$Balance
set.seed(1)
train_ind <- sample(1:nrow(Xmat), floor(0.5*nrow(Xmat)))
set.seed(NULL)
X_mat_train <- Xmat[train_ind,]
X_mat_test <- Xmat[-train_ind,]
y_train <- y[train_ind]
y_test <- y[-train_ind]
#Fit model
lasso.mod <- glmnet(x = X_mat_train, y = y_train,
alpha = 1,
standardize = TRUE)
#Create plot of coefficients
plot(lasso.mod, xvar="lambda",label=TRUE)
#Explore parts of lasso.mod object
lasso.mod$lambda #vector of lambdas chosen by R
dim(coef(lasso.mod)) #dimension of coefficent's estimated
coef_mat <- as.matrix(coef(lasso.mod)) #Create object to hold coefficients
#Explore coefficients for 44th lambda
lasso.mod$lambda[44]
log(lasso.mod$lambda[44])
coef_mat[,44]
#Explore coefficients for 17th lambda
lasso.mod$lambda[17]
log(lasso.mod$lambda[17])
coef_mat[,17]
#Plot
#Create plot of coefficients
plot(lasso.mod, xvar="lambda",label=TRUE)
abline(v=log(lasso.mod$lambda[44]),col="black", lty = "dashed")
abline(v=log(lasso.mod$lambda[17]),col="blue", lty = "dotted")
set.seed (1)
cv.out <- cv.glmnet(x= X_mat_train, y = y_train,
alpha = 1, standardize = TRUE,
nfolds=10)
plot(cv.out)
#Option 1: Pick the lambda that produce the best (min)MSE
bestlam1 <- cv.out$lambda.min
#Predict the responses for the test set (use for MSE calc)
lasso.pred1 <- predict(cv.out , s = bestlam1,
newx = X_mat_test)
#Find the coefficients
lasso.coef1 <- predict(cv.out , s = bestlam1,
type = "coefficients")
bestlam1
lasso.coef1
#Option 2: Pick the largest value of lambda such that error
#is within 1 standard error (1se) of the minimum
bestlam2 <- cv.out$lambda.1se
lasso.pred2 <- predict(cv.out, s = bestlam2,
newx = X_mat_test)
lasso.coef2 <- predict(cv.out, s = bestlam2,
type = "coefficients")
bestlam2
lasso.coef2
knitr::opts_chunk$set(echo = FALSE)
# Load necessary Libraries
library(tidyverse)
library(ggplot2)
library(janitor)
library(kableExtra)
library(glmnet)
# Load Dataset
data <- read.csv("insurance.csv")
knitr::opts_chunk$set(echo = FALSE)
# Load necessary Libraries
library(tidyverse)
library(ggplot2)
library(janitor)
library(kableExtra)
library(glmnet)
# Load Dataset
data <- read.csv("insurance.csv")
Xmat <- model.matrix(charges ~ . , data=data)[ ,-1]
y <- data$charges
set.seed(123)
train_ind <- sample(1:nrow(Xmat), floor(0.8*nrow(Xmat)))
set.seed(NULL)
X_mat_train <- Xmat[train_ind,]
X_mat_test <- Xmat[-train_ind,]
y_train <- y[train_ind]
y_test <- y[-train_ind]
set.seed (123)
cv.out <- cv.glmnet(x= X_mat_train, y = y_train,
alpha = 1, standardize = TRUE,
nfolds=10)
plot(cv.out)
bestlam2 <- cv.out$lambda.1se
lasso.pred2 <- predict(cv.out, s = bestlam2,
newx = X_mat_test)
lasso.coef2 <- predict(cv.out, s = bestlam2,
type = "coefficients")
bestlam2
lasso.coef2
knitr::opts_chunk$set(echo = FALSE)
# Load necessary Libraries
library(tidyverse)
library(ggplot2)
library(janitor)
library(kableExtra)
library(glmnet)
# Load Dataset
data <- read.csv("insurance.csv")
# Check for Null Value
na <- sum(is.na(data))
# Create dummy variable
dummy_data <- data %>%
mutate(
is_male = if_else(sex == "male", 1, 0),
is_smoker = if_else(smoker == "yes", 1, 0),
is_southwest = if_else(region == "southwest", 1, 0),
is_southeast = if_else(region == "southeast", 1, 0),
is_northwest = if_else(region == "northwest", 1, 0),
is_northeast = if_else(region == "northeast", 1, 0)
) %>%
select(-sex, -smoker, -region)
# Create standardized dataset for ML that requires scaling
xvars <- names(dummy_data)
scaled_data <- dummy_data
scaled_data[ , xvars] <- scale(scaled_data[ , xvars],
center = TRUE,
scale = TRUE)
# Create a basic statistic table using the quantitative variables of the dataset.
quant <- c("age", "bmi", "children", "charges")
summary_stat <- psych::describe(data[quant], skew = FALSE) %>%
round(2)
summary_stat %>%
kable(
caption = 'Basic statistics of quantitative variables',
booktabs = TRUE,
align = c('l', rep('c', 8))
) %>%
kableExtra::kable_styling(
bootstrap_options = c('striped', 'condensed'),
font_size = 10,
latex_options = "hold_position"
)
# Doing a 8/2 training, testing splits
set.seed(123)
train_ind <- sample(1:nrow(scaled_data), floor(0.8*nrow(scaled_data)))
set.seed(NULL)
train <- scaled_data[train_ind, ]
test <- scaled_data[-train_ind, ]
# Doing a 8/2 training, testing splits
set.seed(123)
train_ind <- sample(1:nrow(dummy_data), floor(0.8*nrow(dummy_data)))
set.seed(NULL)
train <- dummy_data[train_ind, ]
test <- dummy_data[-train_ind, ]
Xmat <- model.matrix(charges ~ . , data=dummy_data)[ ,-1]
y <- data$charges
set.seed(123)
train_ind <- sample(1:nrow(Xmat), floor(0.8*nrow(Xmat)))
set.seed(NULL)
X_mat_train <- Xmat[train_ind,]
X_mat_test <- Xmat[-train_ind,]
y_train <- y[train_ind]
y_test <- y[-train_ind]
set.seed (123)
cv.out <- cv.glmnet(x= X_mat_train, y = y_train,
alpha = 1, standardize = TRUE,
nfolds=10)
plot(cv.out)
bestlam <- cv.out$lambda.1se
pred_lasso <- predict(cv.out, s = bestlam,
newx = X_mat_test)
coef_lasso <- predict(cv.out, s = bestlam,
type = "coefficients")
RSS_lasso <- sum(dummy_data$charges - pred_lasso)^2
bestlam <- cv.out$lambda.1se
pred_lasso <- predict(cv.out, s = bestlam,
newx = X_mat_test)
coef_lasso <- predict(cv.out, s = bestlam,
type = "coefficients")
bestlam
#RSS_lasso <- sum(dummy_data$charges - pred_lasso)^2
knitr::opts_chunk$set(echo = FALSE)
# Load necessary Libraries
library(tidyverse)
library(ggplot2)
library(janitor)
library(kableExtra)
library(glmnet)
# Load Dataset
data <- read.csv("insurance.csv")
# Check for Null Value
na <- sum(is.na(data))
# Create dummy variable
dummy_data <- data %>%
mutate(
is_male = if_else(sex == "male", 1, 0),
is_smoker = if_else(smoker == "yes", 1, 0),
is_southwest = if_else(region == "southwest", 1, 0),
is_southeast = if_else(region == "southeast", 1, 0),
is_northwest = if_else(region == "northwest", 1, 0),
is_northeast = if_else(region == "northeast", 1, 0)
) %>%
select(-sex, -smoker, -region)
# Create standardized dataset for ML that requires scaling
xvars <- names(dummy_data)
scaled_data <- dummy_data
scaled_data[ , xvars] <- scale(scaled_data[ , xvars],
center = TRUE,
scale = TRUE)
# Doing a 8/2 training, testing splits
set.seed(123)
train_ind <- sample(1:nrow(dummy_data), floor(0.8*nrow(dummy_data)))
set.seed(NULL)
train <- dummy_data[train_ind, ]
test <- dummy_data[-train_ind, ]
Xmat <- model.matrix(charges ~ . , data=dummy_data)[ ,-1]
y <- data$charges
View(Xmat)
set.seed(123)
train_ind <- sample(1:nrow(Xmat), floor(0.8*nrow(Xmat)))
set.seed(NULL)
X_mat_train <- Xmat[train_ind,]
X_mat_test <- Xmat[-train_ind,]
y_train <- y[train_ind]
y_test <- y[-train_ind]
set.seed (123)
cv.out <- cv.glmnet(x= X_mat_train, y = y_train,
alpha = 1, standardize = TRUE,
nfolds=10)
plot(cv.out)
bestlam <- cv.out$lambda.1se
pred_lasso <- predict(cv.out, s = bestlam,
newx = X_mat_test)
coef_lasso <- predict(cv.out, s = bestlam,
type = "coefficients")
bestlam
RSS_lasso <- sum(y_test - pred_lasso)^2
bestlam <- cv.out$lambda.1se
pred_lasso <- predict(cv.out, s = bestlam,
newx = X_mat_test)
coef_lasso <- predict(cv.out, s = bestlam,
type = "coefficients")
RSS_lasso <- sum(y_test - pred_lasso)^2
RSS_lasso
bestlam <- cv.out$lambda.1se
pred_lasso <- predict(cv.out, s = bestlam,
newx = X_mat_test)
coef_lasso <- predict(cv.out, s = bestlam,
type = "coefficients")
RSS_lasso <- mean(sum(y_test - pred_lasso)^2)
RSS_lasso
bestlam <- cv.out$lambda.1se
pred_lasso <- predict(cv.out, s = bestlam,
newx = X_mat_test)
coef_lasso <- predict(cv.out, s = bestlam,
type = "coefficients")
RSS_lasso <- mean(sum(y_test - pred_lasso)^2)
RSS_lasso
bestlam <- cv.out$lambda.1se
pred_lasso <- predict(cv.out, s = bestlam,
newx = X_mat_test)
coef_lasso <- predict(cv.out, s = bestlam,
type = "coefficients")
RSS_lasso <- (y_test - pred_lasso)^2
RSS_lasso
bestlam <- cv.out$lambda.1se
pred_lasso <- predict(cv.out, s = bestlam,
newx = X_mat_test)
coef_lasso <- predict(cv.out, s = bestlam,
type = "coefficients")
RSS_lasso <- mean((y_test - pred_lasso)^2)
RSS_lasso
coef_lasso
Xmat <- model.matrix(charges ~ . , data=scaled_data)[ ,-1]
y <- data$charges
set.seed(123)
train_ind <- sample(1:nrow(Xmat), floor(0.8*nrow(Xmat)))
set.seed(NULL)
X_mat_train <- Xmat[train_ind,]
X_mat_test <- Xmat[-train_ind,]
y_train <- y[train_ind]
y_test <- y[-train_ind]
set.seed (123)
cv.out <- cv.glmnet(x= X_mat_train, y = y_train,
alpha = 1, standardize = TRUE,
nfolds=10)
plot(cv.out)
bestlam <- cv.out$lambda.1se
pred_lasso <- predict(cv.out, s = bestlam,
newx = X_mat_test)
coef_lasso <- predict(cv.out, s = bestlam,
type = "coefficients")
RSS_lasso <- exp(sum((y_test - pred_lasso)^2))
RSS_lasso
bestlam <- cv.out$lambda.1se
pred_lasso <- predict(cv.out, s = bestlam,
newx = X_mat_test)
coef_lasso <- predict(cv.out, s = bestlam,
type = "coefficients")
RSS_lasso <- exp(sum((y_test - pred_lasso)^2))
RSS_lasso
bestlam <- cv.out$lambda.1se
pred_lasso <- predict(cv.out, s = bestlam,
newx = X_mat_test)
coef_lasso <- predict(cv.out, s = bestlam,
type = "coefficients")
RSS_lasso <- sum((y_test - pred_lasso)^2)
RSS_lasso
coef_lasso
Xmat <- model.matrix(charges ~ . , data=scaled_data)[ ,-1]
y <- data$charges
bestlam <- cv.out$lambda.min
pred_lasso <- predict(cv.out, s = bestlam,
newx = X_mat_test)
coef_lasso <- predict(cv.out, s = bestlam,
type = "coefficients")
RSS_lasso <- sum((y_test - pred_lasso)^2)
RSS_lasso
bestlam <- cv.out$lambda.min
pred_lasso <- predict(cv.out, s = bestlam,
newx = X_mat_test)
coef_lasso <- predict(cv.out, s = bestlam,
type = "coefficients")
RSS_lasso <- mean((y_test - pred_lasso)^2)
RSS_lasso
knitr::opts_chunk$set(echo = FALSE)
# Load necessary Libraries
library(tidyverse)
library(ggplot2)
library(janitor)
library(kableExtra)
library(glmnet)
# Load Dataset
data <- read.csv("insurance.csv")
# Check for Null Value
na <- sum(is.na(data))
# Create dummy variable
dummy_data <- data %>%
mutate(
is_male = if_else(sex == "male", 1, 0),
is_smoker = if_else(smoker == "yes", 1, 0),
is_southwest = if_else(region == "southwest", 1, 0),
is_southeast = if_else(region == "southeast", 1, 0),
is_northwest = if_else(region == "northwest", 1, 0),
is_northeast = if_else(region == "northeast", 1, 0)
) %>%
select(-sex, -smoker, -region)
# Create standardized dataset for ML that requires scaling
xvars <- names(dummy_data)
scaled_data <- dummy_data
scaled_data[ , xvars] <- scale(scaled_data[ , xvars],
center = TRUE,
scale = TRUE)
# Doing a 8/2 training, testing splits
set.seed(123)
train_ind <- sample(1:nrow(dummy_data), floor(0.8*nrow(dummy_data)))
set.seed(NULL)
train <- dummy_data[train_ind, ]
test <- dummy_data[-train_ind, ]
scaled_train <- scaled_data[train_ind, ]
scaled_test <- scaled_data[-train_ind, ]
# Building the Multiple Linear Regression model
mlr_model <- lm(charges ~ ., data = scaled_train)
# Viewing the summary of the model
summary(mlr_model)
# Predicting with the test data
pred_mlr <- predict(mlr_model, newdata = scaled_test)
# Calculating Mean Squared Error (MSE) for the test set
MSE_mlr <- mean((test$charges - pred_mlr)^2)
# Displaying the MSE
print(paste("MSE for Multiple Linear Regression: ", MSE_mlr))
# Diagnostic plot - Residuals vs Fitted values
plot(mlr_model$fitted.values, mlr_model$residuals)
abline(h = 0, col = "red")
# Diagnostic plot - QQ plot for residuals
qqnorm(mlr_model$residuals)
qqline(mlr_model$residuals, col = "red")
# Building the Multiple Linear Regression model
mlr_model <- lm(charges ~ ., data = scaled_train)
# Viewing the summary of the model
summary(mlr_model)
# Predicting with the test data
pred_mlr <- exp(predict(mlr_model, newdata = scaled_test))
# Calculating Mean Squared Error (MSE) for the test set
MSE_mlr <- mean((test$charges - pred_mlr)^2)
# Displaying the MSE
print(paste("MSE for Multiple Linear Regression: ", MSE_mlr))
# Building the Multiple Linear Regression model
mlr_model <- lm(charges ~ ., data = scaled_train)
# Viewing the summary of the model
summary(mlr_model)
# Predicting with the test data
pred_mlr <- predict(mlr_model, newdata = scaled_test)
# Calculating Mean Squared Error (MSE) for the test set
MSE_mlr <- mean((scaled_test$charges - pred_mlr)^2)
# Displaying the MSE
print(paste("MSE for Multiple Linear Regression: ", MSE_mlr))
Xmat <- model.matrix(charges ~ . , data=scaled_data)[ ,-1]
y <- scaled_data$charges
set.seed(123)
train_ind <- sample(1:nrow(Xmat), floor(0.8*nrow(Xmat)))
set.seed(NULL)
X_mat_train <- Xmat[train_ind,]
X_mat_test <- Xmat[-train_ind,]
y_train <- y[train_ind]
y_test <- y[-train_ind]
set.seed (123)
cv.out <- cv.glmnet(x= X_mat_train, y = y_train,
alpha = 1, standardize = TRUE,
nfolds=10)
plot(cv.out)
bestlam <- cv.out$lambda.min
pred_lasso <- predict(cv.out, s = bestlam,
newx = X_mat_test)
coef_lasso <- predict(cv.out, s = bestlam,
type = "coefficients")
RSS_lasso <- mean((y_test - pred_lasso)^2)
RSS_lasso
knitr::opts_chunk$set(echo = FALSE)
test <- read.csv("Player and contract.csv")
View(test)
View(test)
getwd()
setwd("C:/Users/wue77/Documents/GitHub/DS-320_Project")
getwd()
install.packages("imager")
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.pos = "H")
library(imager)
getwd()
file <- system.file('/Image//one.png',package='imager')
im <- load.image(file)
file <- system.file('/Image/one.png',package='imager')
im <- load.image(file)
file <- system.file('Image/one.png',package='imager')
im <- load.image(file)
file <- system.file('Users/wue77/Documents/GitHub/DS-320_Project/Image/one.png',package='imager')
im <- load.image(file)
im <- load.image('Users/wue77/Documents/GitHub/DS-320_Project/Image/one.png',package='imager')
im <- load.image('Users/wue77/Documents/GitHub/DS-320_Project/Image/one.png',package='imager')
im <- load.image('Users/wue77/Documents/GitHub/DS-320_Project/Image/one.png')
im <- load.image('Users/wue77/Documents/GitHub/DS-320_Project/Image/one.png')
im <- load.image('Users/wue77/Documents/GitHub/DS-320_Project/Image/one.png')
im <- load.image('Users/wue77/Documents/GitHub/DS-320_Project/Image/one.png')
im <- load.image('Users/wue77/Documents/GitHub/DS-320_Project/Image/one.png')
im <- load.image('Users/wue77/Documents/GitHub/DS-320_Project/Image/one.png')
